# EMNLP 2024 LLM Safety Papers

This repo contains my notes for papers related to LLM safety (e.g., jailbreaks, red-teaming, guardrails).

- ASETF: A Novel Method for Jailbreak Attack on LLMs through Translate Suffix Embeddings
- Alignment-Enhanced Decoding: Defending Jailbreaks via Token-Level Adaptive Refining of Probability Distributions
- Annotation alignment: Comparing LLM and human annotations of conversational safety
- BaitAttack: Alleviating Intention Shift in Jailbreak Attacks via Adaptive Bait Crafting
- ChatGPT Doesnâ€™t Trust Chargers Fans: Guardrail Sensitivity in Context
- CoSafe: Evaluating Large Language Model Safety in Multi-Turn Dialogue Coreference
- Debiasing Text Safety Classifiers through a Fairness-Aware Ensemble
- Defending Jailbreak Prompts via In-Context Adversarial Game
- Distract Large Language Models for Automatic Jailbreak Attack
- GPT-4 Jailbreaks Itself with Near-Perfect Success Using Self-Explanation
- GuardBench: A Large-Scale Benchmark for Guardrail Models
- Large Language Models Are Involuntary Truth-Tellers: Exploiting Fallacy Failure for Jailbreak Attacks
- LoRA-Guard: Parameter-Efficient Guardrail Adaptation for Content Moderation of Large Language Models
- SLM as Guardian: Pioneering AI Safety with Small Language Models
